<!DOCTYPE html>
<html>
<body>
    <h1 align="center">ğŸ‘„ lipsync-mediapipe ğŸ—£ï¸</h1>
    <p align="center">
        <b>lipsync-mediapipe</b> is a project that learns facial landmark shapes from speech audio using Python 3.8. The project uses an innovative approach by mapping melspectrogram slices into 1D arrays of facial landmarks structured as [x1, y1, z1, x2, y2, z2... etc].
    </p>
    <hr>
    <h2>ğŸ’» Setup Environment</h2>
    <ol>
        <li>Ensure your environment is set up with Python 3.8.</li>
        <li>Install the correct PyTorch version from <a href="https://pytorch.org/get-started/locally/">this link</a>.</li>
        <li>Run the following command in your terminal to install the required Python packages:<br>
            <code>pip install -r requirements.txt</code></li>
    </ol>
    <h2>ğŸ—„ï¸ Build Dataset</h2>
    <p>Import a .mov or .mp4 video of a talking head. Ideally, the video should be longer than 10 minutes. You can use the following command line to start building the dataset:</p>
    <pre><code>python dataset.py 'example.mov' 'dataset_name'</code></pre>
    <p>Replace 'example.mov' with the path to your video file and 'dataset_name' with the desired name for your dataset.</p>
    <h2>ğŸš€ Train Model</h2>
    <p>Use the following command line to train the model:</p>
    <pre><code>python train.py 'dataset_name' --batch_size=32 --epochs=30</code></pre>
    <p>Replace 'dataset_name' with the name of your dataset. You can adjust the batch size and the number of epochs as needed.</p>
    <h2>ğŸ“ Note</h2>
    <p>The default batch size is 32, and the default number of epochs is 30. You can adjust these as per your computational capabilities and dataset size.</p>
</body>
</html>
